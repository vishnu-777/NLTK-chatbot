{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZhe535E0M18S4ivflz7Jf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnu-777/NLTK-chatbot/blob/main/NLTK_chatbot_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xUwYvHThHDKm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/data.txt','r',errors = 'ignore') #reading corpus text\n",
        "raw_doc= f.read()"
      ],
      "metadata": {
        "id": "355f0eEjImaD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc = raw_doc.lower() #converting to lowercase\n",
        "nltk.download('punkt') #punkt tokenizer\n",
        "nltk.download('wordnet') #wordnet dictionary\n",
        "nltk.download('omw-1.4')\n",
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBC9h-v5JPG4",
        "outputId": "6a371d17-f6b4-4f14-c922-230efb9f6750"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtmDzoG0KksS",
        "outputId": "1243d6cd-d67a-460c-996e-4732f04a977a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['main', 'menu', 'wikipediathe', 'free', 'encyclopedia']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing steps"
      ],
      "metadata": {
        "id": "KMB7UcCdK-Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n"
      ],
      "metadata": {
        "id": "NGSIfYnBLCiB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Greeting Functions"
      ],
      "metadata": {
        "id": "8Wk0WyXKL_lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs = ('hello','hi','whassup','how are you','heyy','good morning')\n",
        "greet_responses = ('hi','Hey','Hey There!!','heyy hows your day')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "b7OyC_ljL52R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation by the Bot"
      ],
      "metadata": {
        "id": "oc8TDtXQM9bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "RoL9eFd9NA1I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response = ''\n",
        "  TfidVec = TfidfVectorizer(tokenizer= LemNormalize ,stop_words = 'english')\n",
        "  tfidf = TfidVec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if (req_tfidf == 0):\n",
        "    robo1_response = robo1_response + \"I am sorry.Unable to understand you!\"\n",
        "  else:\n",
        "    robo1_response =robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "g89lM9iDaDaP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining ChatFlow"
      ],
      "metadata": {
        "id": "379GMEsabcpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print('Hello I am the Learning bot. Start typing your text after greeting to talk to me. For ending convo type bye!')\n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if (user_response != 'bye'):\n",
        "    if (user_response == 'thank you' or user_response == 'thanks'):\n",
        "      flag= False\n",
        "      print('Bot: You are Welcome...')\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print('Bot '+ greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print('Bot: ',end = '')\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag= False\n",
        "    print('Bot: Goodbye')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzmjGAsTbbUU",
        "outputId": "2b595a50-2006-4840-9b5c-41eeead1aa3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello I am the Learning bot. Start typing your text after greeting to talk to me. For ending convo type bye!\n",
            "heyy\n",
            "Bot Hey\n",
            "hey\n",
            "Bot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "who is alan turing\n",
            "Bot: [7]\n",
            "\n",
            "background\n",
            "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[9] which proposed what is now called the turing test as a criterion of intelligence.\n",
            "who is eliza\n",
            "Bot: development\n",
            "among the most notable early chatbots are eliza (1966) and parry (1972).\n",
            "write the limitations pf chatbot\n",
            "Bot: [81][82]\n",
            "\n",
            "limitations of chatbots\n",
            "the creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases.\n",
            "bye\n",
            "Bot: Goodbye\n"
          ]
        }
      ]
    }
  ]
}